{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSF 619 - Python Code\n",
    "\n",
    "Aaron Leung <br>\n",
    "Andy Wu <br>\n",
    "Tanner Litwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the University of Calgary Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"60:00\" #60 min \n",
    "os.environ['SBATCH_PARTITION']='single' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of Twitter Data By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in full dataset and filtering by location (US and Canada) then writing back out to JSON Files\n",
    "# so we don't have to read in the full 50+ GB file each time\n",
    "\n",
    "print(\"Reading in Full Twitter Dataset\")\n",
    "full_tweets = sqlCtx.read.json(\"./Twitter_Dataset.json\")\n",
    "\n",
    "print(\"Done Reading in Dataset\")\n",
    "print (\"Total Number of Tweets: \", full_tweets.count())\n",
    "\n",
    "print(\"Starting Location Filtering\")\n",
    "loc_filtered_tweets = full_tweets.filter(full_tweets['place']['country_code'].isin(['CA','US']))\n",
    "\n",
    "print(\"Finished Location Filtering\")\n",
    "print(\"Number of Tweets Remaining: \", loc_filtered_tweets.count())\n",
    "\n",
    "print(\"Writing to JSON\")\n",
    "loc_filtered_tweets.write.json(\"LocationFilteredTweets\")\n",
    "\n",
    "print(\"Done Writing to JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Filtering Down to Alberta Only Tweets for Classification\n",
    "\n",
    "loc_filtered_tweets = loc_filtered_tweets.filter(full_tweets['place']['full_name'].contains('Alberta'))\n",
    "\n",
    "print(\"Number of Tweets Remaining: \", loc_filtered_tweets.count())\n",
    "\n",
    "print(\"Writing to JSON\")\n",
    "loc_filtered_tweets.write.json(\"AlbertaLocationFilteredTweets10\")\n",
    "\n",
    "print(\"Done Writing to JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of n-grams from Yelp to build Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Yelp Businesses JSON File as a PySpark Dataframe Object\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "\n",
    "print(\"Reading in Business JSON File\")\n",
    "yelp_business = sqlCtx.read.json(\"business.json\")\n",
    "yelp_business.createOrReplaceTempView(\"business\")\n",
    "\n",
    "print(\"Number of Yelp Businesses: \", yelp_business.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out to only food related businesses\n",
    "\n",
    "yelp_foods = yelp_business.filter(yelp_business[\"categories\"].contains(\"Restaurants\") \\\n",
    "                                 | yelp_business[\"categories\"].contains(\"Food\"))\n",
    "\n",
    "print(\"Food Related Businesses: \", yelp_foods.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Yelp Reviews of all businesses\n",
    "\n",
    "yelp_reviews = sqlCtx.read.json(\"./review.json\")\n",
    "yelp_reviews.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "print(\"Number of Yelp Reviews: \", yelp_reviews.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join on the two dataframes to get all reviews of food related restaurants\n",
    "# Renaming 2 columns so they don't conflict with the other dataframe when joining\n",
    "\n",
    "yelp_reviews = yelp_reviews.withColumnRenamed(\"business_id\", \"review_business_id\") \\\n",
    "                .withColumnRenamed(\"stars\", \"review_stars\")\n",
    "\n",
    "food_reviews = yelp_foods.join(yelp_reviews, yelp_foods.business_id == \\\n",
    "                              yelp_reviews.review_business_id, how=\"inner\")\n",
    "\n",
    "print(food_reviews.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if Reviews match the restaurants\n",
    "\n",
    "for item in food_reviews.select(\"business_id\", \"review_business_id\").take(10):\n",
    "    print(item[\"business_id\"], \" \", item[\"review_business_id\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PySpark DataFrame of all the text reviews then turning it into RDD\n",
    "\n",
    "only_reviews = food_reviews.select(\"text\").rdd\n",
    "\n",
    "# Comes back as RDD but the object is a PySpark Row Object.\n",
    "# To Access String, access [\"text\"] of the row object\n",
    "print(type(only_reviews))\n",
    "print(type(only_reviews.first()))\n",
    "print(type(only_reviews.first()[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Bi-Grams from the reviews\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "treated_review = only_reviews.map(lambda line: line[\"text\"].replace(\"\\n\", \" \").strip().lower())\n",
    "treated_review = treated_review.map(lambda line: \"\".join([x for x in line if x not in string.punctuation]))\n",
    "treated_review = treated_review.map(lambda line: line.strip().split(\" \"))\n",
    "treated_review = treated_review.map(lambda wl: [w for w in wl if len(w) > 0])\n",
    "treated_review = treated_review.map(lambda wl: [w for w in wl if w not in stop_words])\n",
    "\n",
    "bi_grams = treated_review.flatMap(lambda low: [(low[x],low[x+1]) for x in range(0, len(low)-1)])\n",
    "tri_grams = treated_review.flatMap(lambda low: [(low[x-1],low[x],low[x+1]) for x in range (1, len(low)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting up the bi-grams and taking the top 1000 bi-grams\n",
    "\n",
    "bg_count = bi_grams.map(lambda bg: (bg,1)).reduceByKey(lambda i,j: i+j)\n",
    "ordered_bg = bg_count.takeOrdered(1000, lambda x: -1*x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the Bi-grams out to a text file\n",
    "\n",
    "file_out = open(\"bi_gram_counts.txt\", \"w\")\n",
    "\n",
    "print(\"Start Writing to File...\")\n",
    "\n",
    "for bg in ordered_bg:\n",
    "    file_out.write(bg[0][0] + \" \" + bg[0][1] + \" \" + str(bg[1]) + \"\\n\")\n",
    "    \n",
    "print(\"Done Writing\")\n",
    "file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of Twitter Data Based on Context (Keywords/REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX Expressions to Filter Tweets\n",
    "\n",
    "import re\n",
    "\n",
    "def context_filter(tweet):\n",
    "    \n",
    "    first_pattern = \"(food poison|foodpoison)+\"\n",
    "    sec_pattern = ('(inedible|great|terrible|good|fast|mexican|amazing|delicious|chinese|thai|excellent|order|gross|'\n",
    "    'indian|love|awesome|amaze|italian|excellent|fantastic|disgust|incredible|perfect|super|unbelievable|stellar|remarkable|'\n",
    "    'outstanding|bad|nasty|sour|tasteless|rancid|stale|appetizing|appealing|bake|bland|burnt|cold|deep fried|delicious|'\n",
    "    'edible|flavor|fresh|frozen|greasy|grill|prepared|rotten|taste|yummy|quality|ok|sick|puke|eat|ate|ingest|swallow|raw|pink|'\n",
    "    'vomit|cook)+.*(food|restaurant|drink|dine|dining)+')  \n",
    "    \n",
    "    third_pattern = ('(food|restaurant|drink|dine|dining)+.*(court|inedible|great|terrible|good|fast|mexican|amazing|gross|'\n",
    "    'delicious|chinese|thai|excellent|order|indian|love|awesome|amaze|italian|excellent|fantastic|incredible|perfect|super|'\n",
    "    'unbelievable|stellar|remarkable|outstanding|bad|nasty|sour|tasteless|rancid|stale|appetizing|appealing|bake|bland|'\n",
    "    'burnt|cold|deep fried|delicious|edible|flavor|disgust|fresh|frozen|greasy|sick|puke|vomit|eat|ate|ingest|swallow|raw|pink|'\n",
    "    'grill|prepared|rotten|taste|yummy|quality|ok|cook)+')\n",
    "    \n",
    "    fourth_pattern = ('(pizza|burger|sushi|steak|barbecue|bbq|salad|taco|fajita|sandwich|cereal|popcorn|burrito|smores|'\n",
    "    'chimichangas|soup|pepperoni|bratwurst|meatball|fries|cookie|macaroni|nacho|cake|biscuit|gravy|fruit|vegetable)+')\n",
    "    \n",
    "    text = tweet[\"text\"].lower()\n",
    "    \n",
    "    for num in range(1,5):\n",
    "        if num == 1:\n",
    "            result = re.search(first_pattern,text)\n",
    "        elif num == 2:\n",
    "            result = re.search(sec_pattern,text)\n",
    "        elif num == 3:\n",
    "            result = re.search(third_pattern,text)\n",
    "        else:\n",
    "            result = re.search(fourth_pattern,text)\n",
    "            \n",
    "        if result != None:\n",
    "            return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the US/Canada only tweets back into Spark and converting it into an RDD\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "\n",
    "print(\"Staring to read in JSON Files..\")\n",
    "tweets = sqlCtx.read.json(\"./LocationFilteredTweets2\")\n",
    "tweets.createOrReplaceTempView(\"loc_filtered\")\n",
    "\n",
    "print(\"Done Reading in JSON Files..\")\n",
    "\n",
    "tweets_rdd = tweets.rdd\n",
    "\n",
    "# Filtering the Tweets using RDD operations\n",
    "\n",
    "context_filtered = tweets_rdd.filter(lambda rowobj: context_filter(rowobj))\n",
    "context_filtered = context_filtered.filter(lambda rowobj: rowobj[\"retweet_count\"] < 10)\n",
    "filtered = context_filtered.map(lambda x: (x[\"text\"],x[\"place\"][\"full_name\"],x[\"place\"][\"bounding_box\"][\"coordinates\"]))\n",
    "                                \n",
    "\n",
    "print(\"Filtering Process Completed..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis + Formatting For LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "tweetTknzer = TweetTokenizer()\n",
    "sentAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_links = filtered.map(lambda x: (\" \".join([w for w in x[0].split(\" \") if w.startswith(\"https://\") == False]),\\\n",
    "                                        x[0],x[1],x[2])) \n",
    "\n",
    "sentAnalyzed = text_no_links.map(lambda x: (sentAnalyzer.polarity_scores(x[0]),x[1],x[2],x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long (coord):\n",
    "    lat = (coord[0][1] + coord[1][1])/2\n",
    "    long = (coord[0][0] + coord[2][0])/2\n",
    "    return (lat,long)\n",
    "\n",
    "final_out = sentAnalyzed.map(lambda x: (x[1],x[0]['compound'],x[2],lat_long(x[3][0])))\n",
    "final_out = final_out.map(lambda x: (x[0].replace(\"\\n\", \" \"),x[1],x[2],x[3]))\n",
    "final_out = final_out.map(lambda x: (x[0].replace(\"\\t\", \" \"),x[1],x[2],x[3]))\n",
    "final_out = final_out.map(lambda x: (x[0].replace('\"', ''),x[1],x[2],x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sent = final_out.filter(lambda x: x[1] >= 0.05)\n",
    "neutral_sent = final_out.filter(lambda x: x[1] > -0.05 and x[1] < 0.05)\n",
    "negative_sent = final_out.filter(lambda x: x[1] <= -0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Tweets Out to Individual Files For LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return '\\t'.join(str(d) for d in data)\n",
    "\n",
    "fout = open(\"Twitter_31_Info\", \"w\")\n",
    "header = \"Tweet31_\"\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "print(\"Writing Positive Tweets\\n\")\n",
    "for tweet in positive_sent.collect():\n",
    "    string = header + \"positive_\" + str(counter) + \"\\t\" + tweet[0] + \"\\t\" + str(tweet[1]) + \"\\t\" + str(tweet[2]) + \"\\t\" + str(tweet[3]) + \"\\n\"\n",
    "    fout.write(string)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Done Writing Positive Tweets\\n\")\n",
    "\n",
    "print(\"Writing Neutral Tweets\\n\")\n",
    "\n",
    "for tweet in neutral_sent.collect():\n",
    "    string = \"Neutral Stuff \\n\"\n",
    "    fout.write(string)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Done Writing Neutral Tweets\\n\")\n",
    "\n",
    "print(\"Writing Negative Tweets\\n\")\n",
    "for tweet in negative_sent.collect():\n",
    "    string = header + \"negative_\" + str(counter) + \"\\t\" + tweet[0] + \"\\t\" + str(tweet[1]) + \"\\t\" + str(tweet[2]) + \"\\t\" + str(tweet[3]) + \"\\n\"\n",
    "    fout.write(string)\n",
    "    counter += 1 \n",
    "\n",
    "print(\"Done Writing Negative Tweets\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Filtering + Information Extraction + Formatting For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the Alberta Location Filtered Tweets. \n",
    "# Context Filtering is the same as above for LDA (Same REGEX Statements)\n",
    "# The information outputted here is different than that of LDA\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('aggs').getOrCreate()\n",
    "\n",
    "print(\"Staring to read in JSON Files..\")\n",
    "tweets = sqlCtx.read.json(\"./AlbertaLocationFilteredTweets10\")\n",
    "tweets.createOrReplaceTempView(\"loc_filtered\")\n",
    "\n",
    "print(\"Done Reading in JSON Files..\")\n",
    "\n",
    "tweets_rdd = tweets.rdd\n",
    "\n",
    "# Filtering the Tweets using RDD operations\n",
    "\n",
    "context_filtered = tweets_rdd.filter(lambda rowobj: context_filter(rowobj))\n",
    "context_filtered = context_filtered.filter(lambda rowobj: rowobj[\"retweet_count\"] < 10)\n",
    "filtered = context_filtered.map(lambda x: (x[\"text\"],x[\"place\"][\"full_name\"],x[\"place\"][\"bounding_box\"][\"coordinates\"],\\\n",
    "                                          x[\"created_at\"],x[\"place\"][\"place_type\"]))\n",
    "                                \n",
    "print(\"Filtering Process Completed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "tweetTknzer = TweetTokenizer()\n",
    "sentAnalyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_links = filtered.map(lambda x: (\" \".join([w for w in x[0].split(\" \") if w.startswith(\"https://\") == False]),\\\n",
    "                                        x[0],x[1],x[2],x[3],x[4])) \n",
    "\n",
    "sentAnalyzed = text_no_links.map(lambda x: (sentAnalyzer.polarity_scores(x[0]),x[1],x[2],x[3],x[4],x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long (coord):\n",
    "    lat = (coord[0][1] + coord[1][1])/2\n",
    "    long = (coord[0][0] + coord[2][0])/2\n",
    "    return (lat,long)\n",
    "\n",
    "def date (date_time):\n",
    "    split = date_time.split(\" \")\n",
    "    date = str(split[1]) + str(split[2]) + str(split[5])\n",
    "    return date\n",
    "\n",
    "def time(date_time):\n",
    "    split = date_time.split(\" \")\n",
    "    time = str(split[3])\n",
    "    return time\n",
    "\n",
    "final_out = sentAnalyzed.map(lambda x: (x[1],x[0]['compound'],x[2],lat_long(x[3][0]),date(x[4]),time(x[4]),x[5]))\n",
    "\n",
    "final_out = final_out.map(lambda x: (x[0].replace(\"\\n\", \" \"),x[1],x[2],x[3],x[4],x[5],x[6]))\n",
    "final_out = final_out.map(lambda x: (x[0].replace(\"\\t\", \" \"),x[1],x[2],x[3],x[4],x[5],x[6]))\n",
    "final_out = final_out.map(lambda x: (x[0].replace('\"', ''),x[1],x[2],x[3],x[4],x[5],x[6]))\n",
    "\n",
    "positive_sent = final_out.filter(lambda x: x[1] >= 0.05)\n",
    "neutral_sent = final_out.filter(lambda x: x[1] > -0.05 and x[1] < 0.05)\n",
    "negative_sent = final_out.filter(lambda x: x[1] <= -0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Tweet, Compound Score, Location, Coordinates, Date, Time, Place Type into a Tab Separated Value File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return '\\t'.join(str(d) for d in data)\n",
    "\n",
    "tsv_fout = open(\"AB_Twitter_10_TSV\", \"w\")\n",
    "header = \"Tweet10_\"\n",
    "\n",
    "indv_file_path = \"./AB_Tweets_10/Tweet10_\"\n",
    "\n",
    "counter = 0\n",
    "\n",
    "print(\"Writing Positive Tweets\\n\")\n",
    "\n",
    "for tweet in positive_sent.collect():\n",
    "    \n",
    "    string = header + \"positive_\" + str(counter) + \"\\t\" + tweet[0] + \"\\t\" + str(tweet[1]) + \"\\t\" + str(tweet[2]) + \"\\t\"\\\n",
    "    + str(tweet[3]) + \"\\t\" + str(tweet[4]) + \"\\t\" + str(tweet[5]) + \"\\t\" + str(tweet[6]) + \"\\n\"\n",
    "    tsv_fout.write(string)\n",
    "    \n",
    "    indv_tweet = open(indv_file_path + \"positive_\" + str(counter), \"w\")\n",
    "    indv_tweet.write(tweet[0])\n",
    "    indv_tweet.close()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"Done Writing Positive Tweets\\n\")\n",
    "\n",
    "print(\"Writing Neutral Tweets\\n\")\n",
    "\n",
    "for tweet in neutral_sent.collect():\n",
    "    \n",
    "    string = header + \"neutral_\" + str(counter) + \"\\t\" + tweet[0] + \"\\t\" + str(tweet[1]) + \"\\t\" + str(tweet[2]) + \"\\t\"\\\n",
    "    + str(tweet[3]) + \"\\t\" + str(tweet[4]) + \"\\t\" + str(tweet[5]) + \"\\t\" + str(tweet[6]) + \"\\n\"\n",
    "    tsv_fout.write(string)\n",
    "    \n",
    "    indv_tweet = open(indv_file_path + \"neutral_\" + str(counter), \"w\")\n",
    "    indv_tweet.write(tweet[0])\n",
    "    indv_tweet.close()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"Done Writing Neutral Tweets\\n\")\n",
    "\n",
    "print(\"Writing Negative Tweets\\n\")\n",
    "\n",
    "for tweet in negative_sent.collect():\n",
    "    \n",
    "    string = header + \"negative_\" + str(counter) + \"\\t\" + tweet[0] + \"\\t\" + str(tweet[1]) + \"\\t\" + str(tweet[2]) + \"\\t\"\\\n",
    "    + str(tweet[3]) + \"\\t\" + str(tweet[4]) + \"\\t\" + str(tweet[5]) + \"\\t\" + str(tweet[6]) + \"\\n\"\n",
    "    tsv_fout.write(string)\n",
    "    \n",
    "    indv_tweet = open(indv_file_path + \"negative_\" + str(counter), \"w\")\n",
    "    indv_tweet.write(tweet[0])\n",
    "    indv_tweet.close()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "print(\"Done Writing Negative Tweets\\n\")\n",
    "\n",
    "tsv_fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
